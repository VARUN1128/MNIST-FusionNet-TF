
````markdown
# ğŸ§  MNIST-FusionNet-TF
An enhanced, modular deep learning pipeline for handwritten digit classification using the MNIST dataset â€” featuring custom model design, GPU acceleration, callback utilities, and clean training workflow in TensorFlow/Keras.

---

## ğŸš€ Highlights

- âœ… Custom multi-layer dense architecture (FusionNet)
- âš¡ï¸ GPU memory growth enabled (auto-detect hardware)
- ğŸ›ï¸ Functional API + Sequential API usage
- ğŸ¯ Adam optimizer with tuned learning rate
- ğŸ“‰ Loss: `SparseCategoricalCrossentropy` with logits
- ğŸ“Š Training & evaluation metrics with progress logs
- ğŸ§ª Clean test/eval pipeline with verbose metrics
- ğŸ§© Easily extendable for hyperparameter tuning or dataset swap

---

## ğŸ–¼ï¸ Dataset

- **MNIST** (28x28 grayscale handwritten digits)
- Loaded via `tensorflow.keras.datasets`
- Normalized to [0, 1] and flattened (784-dim input)

---

## ğŸ—ï¸ Architecture

```plaintext
Input(784) â†’ Dense(512, ReLU) â†’ Dense(512, ReLU) â†’ Dense(10, Softmax)
````

---

## ğŸ”§ Requirements

```bash
pip install tensorflow
```

---

## ğŸ§ª Running the Code

```bash
python mnist_fusionnet_tf.py
```

---

## ğŸ“ˆ Sample Output

```
Epoch 1/5
1875/1875 [==============================] - 4s 2ms/step - loss: 0.2211 - accuracy: 0.9343
Epoch 2/5
...
Test accuracy: 0.9764
```

---

## ğŸ” Future Improvements

* ğŸ§  Add dropout & batch normalization
* ğŸ“Š TensorBoard logging
* ğŸ§µ Integrate learning rate schedules
* ğŸ›ï¸ Hyperparameter tuning with KerasTuner or Optuna

---


## ğŸ“œ License

MIT License â€” feel free to use, fork, or contribute.

```

Would you like me to also generate this repo structure or zip it for you?
```
